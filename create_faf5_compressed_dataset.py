#!/usr/bin/env python3
"""
FAF5.7 Dataset Compression Script
Creates a comprehensive snapshot of the full FAF5.7 dataset under 100MB
while preserving key patterns, distributions, and diversity.

Author: Supply Chain Analytics Team
Date: 2024
"""

import pandas as pd
import numpy as np
import os
from datetime import datetime

def estimate_file_size_mb(df):
    """Estimate CSV file size in MB"""
    return df.memory_usage(deep=True).sum() / (1024 * 1024) * 1.5  # CSV overhead factor

def stratified_sample_by_column(df, column, n_per_category=None, min_samples=10):
    """
    Create stratified sample ensuring representation from each category
    """
    if n_per_category is None:
        value_counts = df[column].value_counts()
        # Proportional sampling with minimum threshold
        total_target = min(400000, len(df) // 3)  # Target around 400k or ~33% of data
        n_per_category = max(min_samples, total_target // len(value_counts))
    
    sampled_dfs = []
    for category in df[column].unique():
        category_df = df[df[column] == category]
        sample_size = min(n_per_category, len(category_df))
        if sample_size > 0:
            sampled_dfs.append(category_df.sample(n=sample_size, random_state=42))
    
    return pd.concat(sampled_dfs, ignore_index=True)

def get_extreme_values(df, columns, percentile=0.90):
    """
    Get records with extreme values (high/low) for important metrics
    """
    extreme_records = []
    
    for col in columns:
        if col in df.columns and pd.api.types.is_numeric_dtype(df[col]):
            # Get high values
            high_threshold = df[col].quantile(percentile)
            high_values = df[df[col] >= high_threshold]
            extreme_records.append(high_values.sample(n=min(3000, len(high_values)), random_state=42))
            
            # Get low values (non-zero for meaningful metrics)
            low_threshold = df[col].quantile(1 - percentile)
            low_values = df[df[col] <= low_threshold]
            extreme_records.append(low_values.sample(n=min(3000, len(low_values)), random_state=42))
    
    return pd.concat(extreme_records, ignore_index=True).drop_duplicates()

def create_comprehensive_sample(df, target_size_mb=100):
    """
    Create a comprehensive sample using multiple sampling strategies
    """
    print(f"üîç Original dataset: {len(df):,} records, {df.shape[1]} columns")
    print(f"üìè Estimated original size: {estimate_file_size_mb(df):.1f} MB")
    
    # Strategy 1: Stratified sampling by key dimensions
    print("\nüìä Strategy 1: Stratified sampling by key dimensions...")
    
    # Sample by origin state (ensuring geographic diversity)
    sample_by_origin = stratified_sample_by_column(df, 'dms_origst', n_per_category=5000)
    print(f"   - By origin state: {len(sample_by_origin):,} records")
    
    # Sample by commodity type (ensuring commodity diversity)
    sample_by_commodity = stratified_sample_by_column(df, 'sctg2', n_per_category=4000)
    print(f"   - By commodity: {len(sample_by_commodity):,} records")
    
    # Sample by transportation mode
    sample_by_mode = stratified_sample_by_column(df, 'dms_mode', n_per_category=15000)
    print(f"   - By transport mode: {len(sample_by_mode):,} records")
    
    # Sample by distance band
    sample_by_distance = stratified_sample_by_column(df, 'dist_band', n_per_category=20000)
    print(f"   - By distance band: {len(sample_by_distance):,} records")
    
    # Strategy 2: High-value/high-volume corridors
    print("\nüí∞ Strategy 2: High-value and high-volume corridors...")
    
    # Top value corridors
    high_value = df.nlargest(25000, 'value_2023')
    high_tons = df.nlargest(25000, 'tons_2023')
    high_tmiles = df.nlargest(25000, 'tmiles_2023')
    
    print(f"   - High value corridors: {len(high_value):,} records")
    print(f"   - High tonnage corridors: {len(high_tons):,} records")
    print(f"   - High ton-miles corridors: {len(high_tmiles):,} records")
    
    # Strategy 3: Extreme values and outliers
    print("\nüìà Strategy 3: Extreme values and statistical outliers...")
    
    extreme_columns = ['tons_2023', 'value_2023', 'tmiles_2023', 'tons_2017', 'value_2017']
    extreme_values = get_extreme_values(df, extreme_columns)
    
    print(f"   - Extreme values: {len(extreme_values):,} records")
    
    # Strategy 4: Time series diversity (different years)
    print("\nüìÖ Strategy 4: Temporal diversity sampling...")
    
    # Sample records with interesting time patterns
    
    # Records with high growth
    if 'tons_2017' in df.columns and 'tons_2023' in df.columns:
        df_temp = df[(df['tons_2017'] > 0) & (df['tons_2023'] > 0)].copy()
        df_temp['growth_rate'] = (df_temp['tons_2023'] - df_temp['tons_2017']) / df_temp['tons_2017']
        high_growth = df_temp.nlargest(15000, 'growth_rate')
        low_growth = df_temp.nsmallest(15000, 'growth_rate')
        print(f"   - High growth corridors: {len(high_growth):,} records")
        print(f"   - Declining corridors: {len(low_growth):,} records")
    else:
        high_growth = pd.DataFrame()
        low_growth = pd.DataFrame()
    
    # Strategy 5: Medium-value corridors (fill the gap between extremes)
    print("\nüéØ Strategy 5: Medium-value corridors for comprehensive coverage...")
    medium_value = df.nlargest(60000, 'value_2023').tail(35000)  # Get ranks 25001-60000
    medium_tons = df.nlargest(60000, 'tons_2023').tail(35000)
    print(f"   - Medium value corridors: {len(medium_value):,} records")
    print(f"   - Medium tonnage corridors: {len(medium_tons):,} records")
    
    # Strategy 6: Origin-Destination pair diversity
    print("\nüó∫Ô∏è  Strategy 6: Origin-Destination corridor diversity...")
    # Sample by unique origin-destination pairs to ensure geographic diversity
    corridor_sample = stratified_sample_by_column(df, 'dms_origst', n_per_category=2000)
    od_pairs = df.groupby(['dms_origst', 'dms_destst']).apply(lambda x: x.sample(n=min(200, len(x)), random_state=42))
    od_sample = od_pairs.reset_index(drop=True)
    print(f"   - Corridor diversity sample: {len(corridor_sample):,} records")
    print(f"   - Origin-Destination pairs: {len(od_sample):,} records")
    
    # Strategy 7: Random sample for general representation
    print("\nüé≤ Strategy 7: Random sampling for general representation...")
    random_sample = df.sample(n=120000, random_state=42)
    print(f"   - Random sample: {len(random_sample):,} records")
    
    # Combine all samples
    print("\nüîó Combining all sampling strategies...")
    all_samples = [
        sample_by_origin,
        sample_by_commodity, 
        sample_by_mode,
        sample_by_distance,
        high_value,
        high_tons,
        high_tmiles,
        extreme_values,
        high_growth,
        low_growth,
        medium_value,
        medium_tons,
        corridor_sample,
        od_sample,
        random_sample
    ]
    
    # Remove empty dataframes
    all_samples = [sample for sample in all_samples if len(sample) > 0]
    
    # Combine and remove duplicates
    combined_sample = pd.concat(all_samples, ignore_index=True)
    combined_sample = combined_sample.drop_duplicates()
    
    print(f"   - Combined sample: {len(combined_sample):,} records")
    
    # Check size and adjust if needed
    estimated_size = estimate_file_size_mb(combined_sample)
    print(f"   - Estimated size: {estimated_size:.1f} MB")
    
    if estimated_size > target_size_mb:
        print(f"\n‚öñÔ∏è  Adjusting sample size to meet {target_size_mb}MB target...")
        # Calculate reduction ratio
        reduction_ratio = target_size_mb / estimated_size
        target_records = int(len(combined_sample) * reduction_ratio)
        
        # Stratified reduction to maintain diversity
        final_sample = combined_sample.sample(n=target_records, random_state=42)
        print(f"   - Final sample: {len(final_sample):,} records")
        print(f"   - Final estimated size: {estimate_file_size_mb(final_sample):.1f} MB")
    else:
        final_sample = combined_sample
    
    return final_sample

def add_sample_metadata(df_sample, df_original):
    """
    Add metadata about the sampling process
    """
    metadata = {
        'original_records': len(df_original),
        'sample_records': len(df_sample),
        'compression_ratio': len(df_sample) / len(df_original),
        'sampling_date': datetime.now().isoformat(),
        'original_columns': df_original.shape[1],
        'sample_columns': df_sample.shape[1]
    }
    
    return metadata

def validate_sample_quality(df_sample, df_original):
    """
    Validate that the sample preserves key characteristics
    """
    print("\n‚úÖ Validating sample quality...")
    
    validation_results = {}
    
    # Check categorical distributions
    categorical_cols = ['dms_origst', 'dms_destst', 'sctg2', 'dms_mode', 'dist_band', 'trade_type']
    
    for col in categorical_cols:
        if col in df_original.columns:
            orig_unique = set(df_original[col].unique())
            sample_unique = set(df_sample[col].unique())
            coverage = len(sample_unique) / len(orig_unique)
            validation_results[f'{col}_coverage'] = coverage
            print(f"   - {col}: {coverage:.1%} category coverage ({len(sample_unique)}/{len(orig_unique)})")
    
    # Check numeric distributions
    numeric_cols = ['tons_2023', 'value_2023', 'tmiles_2023']
    
    for col in numeric_cols:
        if col in df_original.columns:
            orig_stats = df_original[col].describe()
            sample_stats = df_sample[col].describe()
            
            # Check if key percentiles are reasonably preserved
            percentiles = [0.25, 0.5, 0.75, 0.95]
            percentile_diffs = []
            
            for p in percentiles:
                orig_val = df_original[col].quantile(p)
                sample_val = df_sample[col].quantile(p)
                if orig_val > 0:
                    diff = abs(sample_val - orig_val) / orig_val
                    percentile_diffs.append(diff)
            
            avg_diff = np.mean(percentile_diffs) if percentile_diffs else 0
            validation_results[f'{col}_percentile_preservation'] = 1 - avg_diff
            print(f"   - {col}: {(1-avg_diff):.1%} percentile preservation")
    
    return validation_results

def main():
    """
    Main function to create compressed FAF5.7 dataset
    """
    print("üöÄ FAF5.7 Dataset Compression Tool")
    print("=" * 50)
    
    # Load original dataset
    input_file = 'FAF5.7_State.csv'
    output_file = 'FAF5.7_State_Compressed.csv'
    metadata_file = 'FAF5.7_Compression_Metadata.txt'
    
    if not os.path.exists(input_file):
        print(f"‚ùå Error: {input_file} not found!")
        print("   Please ensure the FAF5.7_State.csv file is in the current directory.")
        return
    
    print(f"üìÇ Loading {input_file}...")
    try:
        df_original = pd.read_csv(input_file)
    except Exception as e:
        print(f"‚ùå Error loading file: {e}")
        return
    
    # Create comprehensive sample
    df_compressed = create_comprehensive_sample(df_original, target_size_mb=100)
    
    # Validate sample quality
    validation_results = validate_sample_quality(df_compressed, df_original)
    
    # Add metadata
    metadata = add_sample_metadata(df_compressed, df_original)
    
    # Save compressed dataset
    print(f"\nüíæ Saving compressed dataset to {output_file}...")
    df_compressed.to_csv(output_file, index=False)
    
    actual_size = os.path.getsize(output_file) / (1024 * 1024)
    print(f"‚úÖ Compressed dataset saved!")
    print(f"   - File size: {actual_size:.1f} MB")
    print(f"   - Records: {len(df_compressed):,}")
    print(f"   - Compression ratio: {metadata['compression_ratio']:.1%}")
    
    # Save metadata
    print(f"\nüìã Saving metadata to {metadata_file}...")
    with open(metadata_file, 'w') as f:
        f.write("FAF5.7 Dataset Compression Metadata\n")
        f.write("=" * 40 + "\n\n")
        
        f.write("COMPRESSION SUMMARY:\n")
        for key, value in metadata.items():
            f.write(f"  {key}: {value}\n")
        
        f.write(f"\nACTUAL FILE SIZE: {actual_size:.1f} MB\n")
        
        f.write("\nQUALITY VALIDATION:\n")
        for key, value in validation_results.items():
            f.write(f"  {key}: {value:.1%}\n")
        
        f.write("\nSAMPLING STRATEGY:\n")
        f.write("  1. Stratified sampling by geographic regions\n")
        f.write("  2. Stratified sampling by commodity types\n") 
        f.write("  3. Stratified sampling by transportation modes\n")
        f.write("  4. High-value/high-volume corridor preservation\n")
        f.write("  5. Extreme values and outlier inclusion\n")
        f.write("  6. Temporal diversity (growth patterns)\n")
        f.write("  7. Medium-value corridors for comprehensive coverage\n")
        f.write("  8. Origin-Destination corridor diversity\n")
        f.write("  9. Random sampling for general representation\n")
        
        f.write("\nUSAGE NOTES:\n")
        f.write("  - This compressed dataset preserves key patterns and distributions\n")
        f.write("  - Suitable for exploratory analysis, prototyping, and development\n")
        f.write("  - For production analysis, consider using the full dataset\n")
        f.write("  - Sample maintains geographic, commodity, and modal diversity\n")
    
    print("‚úÖ Metadata saved!")
    
    # Final summary
    print(f"\nüéØ COMPRESSION COMPLETE!")
    print(f"üìä Summary:")
    print(f"   - Original: {len(df_original):,} records")
    print(f"   - Compressed: {len(df_compressed):,} records")
    print(f"   - Size reduction: {(1-actual_size/estimate_file_size_mb(df_original)):.1%}")
    print(f"   - File size: {actual_size:.1f} MB (target: <100 MB)")
    print(f"   - Quality: High (preserves key distributions and patterns)")
    
    print(f"\nüìÅ Files created:")
    print(f"   - {output_file} (compressed dataset)")
    print(f"   - {metadata_file} (compression details)")

if __name__ == "__main__":
    main()